{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Homework 2\n",
    "Student name: Olha Tanyuk\n",
    "\n",
    "1.\tIn Python, create a method for scoring the vocabulary size of a text, and normalize the score from 0 to 1. It does not matter what method you use for normalization as long as you explain it in a short paragraph. (Various methods will be discussed in the live session.)\n",
    "\n",
    "2.\tAfter consulting section 3.2 in chapter 1 of Bird-Klein, create a method for scoring the long-word vocabulary size of a text, and likewise normalize (and explain) the scoring as in step 1 above.\n",
    "\n",
    "3.\tNow create a “text difficulty score” by combining the lexical diversity score from homework 1, and your normalized score of vocabulary size and long-word vocabulary size, in equal weighting. Explain what you see when this score is applied to same graded texts you used in homework 1.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Answers:\n",
    "Before we start with 1st question, we need to clean text that we are working with. We will move the header and the footer out from the text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove header and footer\n",
    "def removeMeta(t):\n",
    "    start_line = \"*** START OF THIS PROJECT GUTENBERG EBOOK\"\n",
    "    end_line = \"*** END OF THIS PROJECT GUTENBERG EBOOK\"\n",
    "    beg = t.find(start_line)\n",
    "    end = t.rfind(end_line)\n",
    "    b = t[beg:end] \n",
    "    return b\n",
    "\n",
    "def lexical_diversity(text):\n",
    "     return len(set(text)) / len(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1=open(\"books/14640.txt\")\n",
    "txt1 =f1.read()\n",
    "txt1_final = removeMeta(txt1)\n",
    "tokens1 = nltk.word_tokenize(txt1_final)\n",
    "txt1_final = nltk.Text(tokens1)\n",
    "lexical_diversity1 = lexical_diversity(txt1_final)\n",
    "vocab1 = len(set(txt1_final))\n",
    "len1 = len(txt1_final)\n",
    "name1 = 'The Project Gutenberg EBook of McGuffeys First Eclectic Reader'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "f2=open(\"books/14766.txt\", \"r\")\n",
    "txt2 =f2.read()\n",
    "txt2_final = removeMeta(txt2)\n",
    "tokens2 = nltk.word_tokenize(txt2_final)\n",
    "txt2_final = nltk.Text(tokens2)\n",
    "lexical_diversity2 = lexical_diversity(txt2_final)\n",
    "vocab2 = len(set(txt2_final))\n",
    "len2 = len(txt2_final)\n",
    "name2 = 'The Project Gutenberg EBook of McGuffeys Third Eclectic Reader'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "f3=open(\"books/16751.txt\", \"r\")\n",
    "txt3 =f3.read()\n",
    "txt3_final = removeMeta(txt3)\n",
    "tokens3 = nltk.word_tokenize(txt3_final)\n",
    "txt3_final = nltk.Text(tokens3)\n",
    "lexical_diversity3 = lexical_diversity(txt3_final)\n",
    "vocab3 = len(set(txt3_final))\n",
    "len3 = len(txt3_final)\n",
    "name3 = 'The Project Gutenberg EBook of McGuffeys Sixth Eclectic Reader'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. \n",
    "The Second Edition of the 20-volume Oxford English Dictionary, published in 1989, contains full entries for 171,476 words in current use. For scoring the vocabulary size of a text and its normalization we will calculate amount of unique tokens of the text and will divide this number by 171,476. In the output we will get the vocabulary size score from 0 to 1, which will represent the % of English words you will need to read this text. We will call this score as \"All Words Score\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Book                                                              Vocabulary size    All Words Score\n",
      "--------------------------------------------------------------  -----------------  -----------------\n",
      "The Project Gutenberg EBook of McGuffeys First Eclectic Reader               1483         0.00864844\n",
      "The Project Gutenberg EBook of McGuffeys Third Eclectic Reader               4170         0.0243183\n",
      "The Project Gutenberg EBook of McGuffeys Sixth Eclectic Reader              16923         0.0986902\n"
     ]
    }
   ],
   "source": [
    "AllWordScore1 = vocab1/171476\n",
    "AllWordScore2 = vocab2/171476\n",
    "AllWordScore3 = vocab3/171476\n",
    "\n",
    "from tabulate import tabulate\n",
    "print(tabulate([[name1, vocab1, AllWordScore1], [name2, vocab2, AllWordScore2], [name3, vocab3, AllWordScore3]], headers=['Book', 'Vocabulary size', 'All Words Score']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.\n",
    "For scoring the long-word vocabulary size of a text we will calculate amount of unique 10 letters words and longer and will divide this number by total unique tokens of the text. In this way we will see the percentage of long words in the text's vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Book                                                              Vocabulary size    Long Word Score\n",
      "--------------------------------------------------------------  -----------------  -----------------\n",
      "The Project Gutenberg EBook of McGuffeys First Eclectic Reader               1483          0.0310182\n",
      "The Project Gutenberg EBook of McGuffeys Third Eclectic Reader               4170          0.0448441\n",
      "The Project Gutenberg EBook of McGuffeys Sixth Eclectic Reader              16923          0.154819\n"
     ]
    }
   ],
   "source": [
    "def longWordCnt(t, min_length = 10):\n",
    "    if t is str:\n",
    "        t = split_text(t)\n",
    "    \n",
    "    longWords = []\n",
    "    \n",
    "    for w in t:\n",
    "        if(len(w) >= min_length):\n",
    "            longWords.append(w)\n",
    "    \n",
    "    return len(set(longWords))\n",
    "    \n",
    "longWordScore1 = longWordCnt(txt1_final)/vocab1\n",
    "longWordScore2 = longWordCnt(txt2_final)/vocab2\n",
    "longWordScore3 = longWordCnt(txt3_final)/vocab3\n",
    "\n",
    "from tabulate import tabulate\n",
    "print(tabulate([[name1, vocab1, longWordScore1], [name2, vocab2, longWordScore2], [name3, vocab3, longWordScore3]], headers=['Book', 'Vocabulary size', 'Long Word Score']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.\n",
    "Now we will create a “text difficulty score” by combining the lexical diversity score, normalized score of vocabulary size and long-word vocabulary size, in equal weighting. When this score was applied to same graded texts I used in homework 1, we got rational picture where higher grade texts have higher Text Difficulty Score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Book                                                              Text Difficulty Score\n",
      "--------------------------------------------------------------  -----------------------\n",
      "The Project Gutenberg EBook of McGuffeys First Eclectic Reader               0.00420271\n",
      "The Project Gutenberg EBook of McGuffeys Third Eclectic Reader               0.013142\n",
      "The Project Gutenberg EBook of McGuffeys Sixth Eclectic Reader               0.154168\n"
     ]
    }
   ],
   "source": [
    "diff1 = lexical_diversity1*AllWordScore1*longWordScore1*100\n",
    "diff2 = lexical_diversity2*AllWordScore2*longWordScore2*100\n",
    "diff3 = lexical_diversity3*AllWordScore3*longWordScore3*100\n",
    "\n",
    "print(tabulate([[name1, diff1], [name2, diff2], [name3, diff3]], headers=['Book', \"Text Difficulty Score\"]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
