{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Homework 6: ELMO\n",
    "\n",
    "#### Student name: Olha Tanyuk\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#requires Tensorflow 2.0 and Tf-hub 0.7!!\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.0.0\n",
      "0.7.0\n"
     ]
    }
   ],
   "source": [
    "print(tf.__version__)\n",
    "print(hub.__version__)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load up the ELMO model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W1115 12:57:56.722823  2808 deprecation.py:506] From C:\\Users\\otany\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\ops\\resource_variable_ops.py:1781: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n"
     ]
    }
   ],
   "source": [
    "module_url = \"https://tfhub.dev/google/elmo/3\"\n",
    "embed = hub.load(module_url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example of how to get data from the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "text=tf.convert_to_tensor([\"I am feeling kind of blue\", \"Blue is the color of the sky\",\"lol wut\"])\n",
    "out = embed.signatures['default'](text)['elmo']\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([3, 7, 1024])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=6281, shape=(1024,), dtype=float32, numpy=\n",
       "array([-0.5775769 , -0.7272091 ,  0.05588115, ...,  0.2270948 ,\n",
       "        0.30447572,  0.3096969 ], dtype=float32)>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out[0][5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Outputs\n",
    "\n",
    "### The output dictionary contains:\n",
    "\n",
    "  * word_emb: the character-based word representations with shape [batch_size, max_length, 512].\n",
    "  * lstm_outputs1: the first LSTM hidden state with shape [batch_size, max_length, 1024].\n",
    "  * lstm_outputs2: the second LSTM hidden state with shape [batch_size, max_length, 1024].\n",
    "  * elmo: the weighted sum of the 3 layers, where the weights are trainable. This tensor has shape [batch_size, max_length, 1024]\n",
    "  * default: a fixed mean-pooling of all contextualized word representations with shape [batch_size, 1024].\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q1:  Run a 4 words with different context meanings through Elmo--what is the (cosine) similarity of that word in the different contexts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the (cosine) similarity for word 'net' with two diff meanings: 0.6077427268028259\n"
     ]
    }
   ],
   "source": [
    "text1=tf.convert_to_tensor([\"What was your net gain for the year\", \"Crabbing is easier if you bring a net along\"])\n",
    "out1 = embed.signatures['default'](text1)['elmo']\n",
    "out1_1 = out1[0][3]\n",
    "out1_2 = out1[1][7]\n",
    "\n",
    "from scipy.spatial import distance\n",
    "print (\"the (cosine) similarity for word 'net' with two diff meanings:\",(distance.cosine(out1_1, out1_2))*(-1)+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the (cosine) similarity for word 'engaged' with two diff meanings: 0.8104850053787231\n"
     ]
    }
   ],
   "source": [
    "text2=tf.convert_to_tensor([\"They got engaged on March 7th\", \"The students were very engaged in the presentation\"])\n",
    "out2 = embed.signatures['default'](text2)['elmo']\n",
    "out2_1 = out2[0][2]\n",
    "out2_2 = out2[1][4]\n",
    "\n",
    "print (\"the (cosine) similarity for word 'engaged' with two diff meanings:\",(distance.cosine(out2_1, out2_2))*(-1)+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the (cosine) similarity for word 'date' with two diff meanings: 0.6532547473907471\n"
     ]
    }
   ],
   "source": [
    "text3=tf.convert_to_tensor([\"Joe took Alexandria out on a date\", \"What is your date of birth\"])\n",
    "out3 = embed.signatures['default'](text3)['elmo']\n",
    "out3_1 = out3[0][6]\n",
    "out3_2 = out3[1][3]\n",
    "\n",
    "print (\"the (cosine) similarity for word 'date' with two diff meanings:\",(distance.cosine(out3_1, out3_2))*(-1)+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the (cosine) similarity for word 'rose' with two diff meanings: 0.45836153626441956\n"
     ]
    }
   ],
   "source": [
    "text4=tf.convert_to_tensor([\"My favorite flower is a rose\", \"He quickly rose from his seat\"])\n",
    "out4 = embed.signatures['default'](text4)['elmo']\n",
    "out4_1 = out4[0][5]\n",
    "out4_2 = out4[1][2]\n",
    "\n",
    "print (\"the (cosine) similarity for word 'rose' with two diff meanings:\",(distance.cosine(out4_1, out4_2))*(-1)+1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q2:  Where is the padding?  Where does it go in the sequence?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=96822, shape=(1024,), dtype=float32, numpy=\n",
       "array([-0.02840841, -0.04353216,  0.04130162, ...,  0.02583168,\n",
       "       -0.01429836, -0.01650422], dtype=float32)>"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out3_3 = out3[1][6]\n",
    "out3_3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=96854, shape=(1024,), dtype=float32, numpy=\n",
       "array([-0.02840841, -0.04353216,  0.04130163, ...,  0.02583168,\n",
       "       -0.01429836, -0.01650423], dtype=float32)>"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out2_3 = out2[0][6]\n",
    "out2_4 = out2[0][7]\n",
    "out2_3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=96862, shape=(1024,), dtype=float32, numpy=\n",
       "array([-0.02840841, -0.04353216,  0.04130163, ...,  0.02583168,\n",
       "       -0.01429836, -0.01650423], dtype=float32)>"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out2_4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks like padding goes at the end of the sequence to match the longest sentence. Interesting, that the values in the padding arrays are the same for each padding for different sentences."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q3 Describe how you would use an ELMO vector in a Neural Network below. Where does ELMO go? What else needs to be changed (added/removed)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy\n",
    "from tensorflow.keras.datasets import imdb\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import  Lambda, LSTM as LSTM, SimpleRNN\n",
    "from tensorflow.keras.layers import Embedding\n",
    "from tensorflow.keras.preprocessing import sequence\n",
    "numpy.random.seed(7)\n",
    "top_words = 5000\n",
    "(X_train, y_train), (X_test, y_test) = imdb.load_data(num_words=top_words)\n",
    "max_review_length = 500\n",
    "X_train = sequence.pad_sequences(X_train, maxlen=max_review_length)\n",
    "X_test = sequence.pad_sequences(X_test, maxlen=max_review_length)\n",
    "\n",
    "# embedding_vecor_length is not 300 any more, we will use 1024 in ElmoEmbedding\n",
    "batch_size=128\n",
    "model2 = Sequential()\n",
    "\n",
    "# Embeding should be ELMO\n",
    "elmo_model = hub.Module(\"https://tfhub.dev/google/elmo/2\", trainable=True)\n",
    "def ElmoEmbedding(x):\n",
    "    return elmo_model(inputs={\n",
    "                            \"tokens\": tf.squeeze(tf.cast(x, tf.string)),\n",
    "                            \"sequence_len\": tf.constant(batch_size*[max_review_length])\n",
    "                      },\n",
    "                      signature=\"tokens\",\n",
    "                      as_dict=True)[\"elmo\"]\n",
    "input_text = Input(shape=(max_review_length,), dtype=tf.string)\n",
    "embedding = Lambda(ElmoEmbedding, output_shape=(None, max_review_length, 1024))(input_text)\n",
    "\n",
    "# Change to Bidirectional LSTM\n",
    "model2.add(Bidirectional(LSTM(512, return_sequences=True)))(embedding)\n",
    "\n",
    "model2.add(Dense(1, activation='sigmoid'))\n",
    "model2.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "print(model2.summary())\n",
    "model2.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=3, batch_size=128)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q4 Compare the size of elmo vectors (dot product) with the size of spacy vectors (dot product)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "text=tf.convert_to_tensor([\"I am feeling kind of blue\", \"Blue is the color of the sky\",\"lol wut\"])\n",
    "out = embed.signatures['default'](text)['elmo']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "94.40007"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# dot product for words \"feeling\" and \"kind\" from one sentence using elmo\n",
    "np.dot(out[0][2],out[0][3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20.884392"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# dot product for words \"feeling\" and \"kind\" using spacy\n",
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_md\")\n",
    "token1 = nlp(\"feeling\")\n",
    "token2 = nlp(\"kind\")\n",
    "\n",
    "np.dot(token1.vector,token2.vector)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The size of spacy vectors (dot product) is smaller than the size of elmo vectors (dot product). Probably one of the reasons is that Elmo vector length is 1024 while the spacy vector length is 300. Lets check another pair of words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "66.9234"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# dot product for words \"blue\" and \"color\" from different sentences using elmo\n",
    "np.dot(out[0][5],out[1][3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "29.746302"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# dot product for words \"blue\" and \"color\" using spacy\n",
    "token3 = nlp(\"blue\")\n",
    "token4 = nlp(\"color\")\n",
    "\n",
    "np.dot(token3.vector,token4.vector)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We got the same result: the size of spacy vectors (dot product) is smaller than the size of elmo vectors (dot product)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
